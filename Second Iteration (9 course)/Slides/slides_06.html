<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>slides_06.knit</title>
    <meta charset="utf-8" />
    <meta name="author" content="Thomas Rosenthal - DSI @ UofT" />
    <script src="libs/header-attrs-2.19/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# <p><img src="imgs/mrpotatocode_banner.png" style="width:7in" /></p>
]
.subtitle[
## Introduction to Data Access and Storage
]
.author[
### Thomas Rosenthal - DSI @ UofT
]
.date[
### Module 06
]

---




class: middle, center, inverse
# SQL and Machine Learning:

## What is Machine Learning?

## SQL's Role

## Run SQLite in python and/or R

## Connecting the Dots

---

class: middle, center, inverse
# SQL and Machine Learning:

## **What is Machine Learning?**

## SQL's Role

## Run SQLite in python and/or R

## Connecting the Dots

---

class: middle, center, inverse
# What is Machine Learning?

## Supervised Learning

## Unsupervised Learning

## Basic Machine Learning Process

---


class: top, left, inverse
# What is Machine Learning?

- Machine Learning (ML) uses *data* to discover *patterns* by the use of *statistics*
- ML is everywhere:
  - **Predictive Models** estimate the outcome based on the relationships of the model's inputs
      - Economic/Financial Studies
      - Health Studies
  - **Classification Models** highlight anomalies, determine how things can be sorted into predefined categories 
      - Image classification ("select the pictures of stairs")
      - Fraudulent credit card transactions
  - **Clustering Models** create groupings based on similarities
      - Customer Groupings
      - News Topics
  - **Recommender Models** suggest what to watch, click, serve, display, etc
      - News Feeds (social media)
      - What to Watch (Netflix, etc)
      - Personalized Ads
  - **Language Models** analyze human speech 
      - Chatbots on websites
      - ChatGPT / GPT3
      - Text Summarization tools
  - **Generative Models** can even create "art"?
      - DALL-E

---

class: top, left, inverse
# What is Machine Learning?

- We can broadly split ML into three types:
  - Supervised Learning
  - Unsupervised Learning
  - Reinforcement Learning (we'll skip this one)

&lt;br&gt;
--

- Supervised: we provide the algorithms with explicit values/labels to base future predictions off of
- Supervised can be split into two tasks:
  - Regression
  - Classification

&lt;br&gt;
--

- Unsupervised: we don't provide the algorithms with explicit values/labels, requiring the machine to create groupings based on other mathematical properties
- Unsupervised can be split into several tasks:
  - Clustering
  - Association
  - Decomposition
  - ...more
  
---

class: middle, center, inverse
# What is Machine Learning?

## **Supervised Learning**

## Unsupervised Learning

## Basic Machine Learning Process

---

class: top, left, inverse
# Supervised Learning

- Regression predicts numerical values
  - How much should this insurance policy cost?
      - Input: driver's experience, maximum financial damages possible, car characteristics, etc
      - Output: predicted premium value

&lt;br&gt;
--

- Classification predicts categories
  - Is this email spam or not?
      - Input: all of the emails from Enron (I'm not joking here), labels generated by humans 
      - Output: "Buy Medication Cheap" --&gt; SPAM!

---

class: top, left, inverse
# Supervised Learning

- Regression Algorithms:
  - Linear regression, predicting values along a linear line
  - Regularized (i.e. Ridge, LASSO) regression, predicting values by penalizing coefficients to reduce model complexity
  - Support Vector Machines (SVM) regression, predicting values within a hyperdimensional space
  - ...and many more

&lt;br&gt;
--

- Classification Algorithms:
  - Logistic regression (despite its name!), predicting labels along a sigmoid (binomial: either 0 or 1 )
  - Decision Tree classification, predicting labels based on information gained by splitting nodes 
  - Support Vector Machines (SVM) classification, predicting labels within a hyperdimensional space
  - ...and many more

---


class: middle, center, inverse
# What is Machine Learning?

## Supervised Learning

## **Unsupervised Learning**

## Basic Machine Learning Process

---

class: top, left, inverse
# Unsupervised Learning

- Clustering objects/concepts (data points) that are most similar to one another
  - Using behavioural data, which customers are similar to one another?
      - Input: behavioural data, other features deemed relevant
      - Output: customer X belongs to group 1, customer Y belongs to group 2, etc
  
&lt;br&gt;    
--

- Association
  - Using purchase history, what things are often bought together?
      - Input: purchases, other features deemed relevant 
      - Output: "fathers, while you're buying diapers, do you also want to buy beer?"

&lt;br&gt;    
--

- Decomposition
  - What combination of features describes the value of a real estate asset (e.g. house, condo, etc)?
      - Input: interior details, exterior details, geographic features, neighbourhood amenities, nearness to transit...many, many more
      - Output: nearness to transit and fewer bedrooms vs quality of schools and parking spaces available (etc, many more)


---

class: top, left, inverse
# Unsupervised Learning

- Clustering algorithms:
  - K-means, discovery of groupings based on a given point's nearness (by use of a distance metric, e.g. Euclidean) to a cluster's center
  - Hierarchical clustering, discovery of groupings based on hierarchy creation (either agglomerative or divisive)
  - ...and a few more
  

&lt;br&gt;
--

- Association algorithms:
  - *a priori* association, discovery of frequent combinations in an itemset (by use of Cartesian joins) 
  - ...and a few more

&lt;br&gt;
--

- Decomposition algorithms:
  - Principal Component Analysis (PCA), discovery of groupings based on maximizing variance between groupings
    - ...and a few more

---

class: middle, center, inverse
**General ML Questions?**

---

class: middle, center, inverse
# What is Machine Learning?

## Supervised Learning

## Unsupervised Learning

## **Basic Machine Learning Process**

---

class: top, left, inverse
# Basic Machine Learning Process 

This process describes supervised learning. Unsupervised learning is similar, but skips comparisons against test datasets.

- Acquire data
- Clean data
- Preprocess data

&lt;br&gt;
--

- Divide data into training, testing, (and validation) datasets
- Using the training dataset, allow the selected algorithm to generate variable relationships (e.g. coefficients)
- Using these coefficients, produce predicted values on the test dataset

&lt;br&gt;
--

- Evaluate the model's performance by comparing predicted values in the test dataset to actual values in the test dataset
- If the model performs well, predict values on data that does not yet have actual values
- Separately, collect and store actual values ("ground truth", e.g. what actually happened), to compare the model's performance over time


---

class: middle, center, inverse
**ML Pipeline Questions?**

---


class: middle, center, inverse
# SQL and Machine Learning:

## What is Machine Learning?

## **SQL's Role**

## SQLite Meets python and/or R

## Connecting the Dots

---

class: top, left, inverse
# SQL's Role

- SQL isn't designed to run ML algorithms, but it can help serve as the foundation for good ML pipelines
  - Dataset reliability will lend itself to higher quality models over time
  - Observing changes in data requires good data architecture (one of SQL's strengths!)
      - We'll revisit this as "Model Monitoring"

&lt;br&gt;
--

- SQL DBs can be connected to python/R fairly easily
  - Preprocessing can be performed in SQL beforehand, in python/R after extraction, or both
  - Instantiating simple preprocessing actions in a permanent table can reduce the need to rerun calculations that would be stored in memory for python/R
      - e.g. if you write several window functions and save these to a table, these values are available for current and future ML pipelines without the need to run calculations again in a seperate python/R script (or if models were rerun)

&lt;br&gt;
--

- Some RMDBs can run python/R scripts within procedures, saving the output in a SQL table
  - SQLite isn't one of these, but there are other ways we'll create similar outcomes

---

class: middle, center, inverse
**Questions about SQL's Role in ML?**

---

class: middle, center, inverse
# SQL and Machine Learning:

## What is Machine Learning?

## SQL's Role

## **SQLite Meets python and/or R**

## Connecting the Dots

---

class: middle, center, inverse
# SQLite Meets python and/or R

## Why python and/or R?

## Connect to a SQLite DB

## Run SQLite Commands 

---

class: middle, center, inverse
# SQLite Meets python and/or R

## **Why python and/or R?**

## Connect to a SQLite DB

## Run SQLite Commands 

---

class: top, left, inverse
# Why python and/or R?

- Python/R are open source, free, and flexible programming languages 
- ML is relatively mature in both:
  - Python: SKLearn serves as a great foundation for the most common algorithms
      - The cutting edge for ML is generally in python: PyTorch, TensorFlow, other Deep Learning algorithms
  - R: a bit more of a cobbled together history (caret, clustering packages, etc), but now well served by tidymodels
      - R is designed for statistics...building customized modelling approaches may be easier for statisticians than python (less programming overhead)

&lt;br&gt;
--

- Both python/R can connect to SQLite!
- Both python/R can **run** SQLite queries!

&lt;br&gt;
---

class: top, left, inverse
# Why python and/or R?

- Python/R can also be instrumental in data engineering
- Previously, it was more challenging to move data between SQL systems
    - We'd use ETL tools like SQL Server Integration Services (SSIS), establish our connections to each SQL system, perform whatever transformations needed to be done (e.g. normalization), and load the result sets into the appropriate corresponding tables
    - The overhead was challenging to maintain
    - The tools often weren't free

&lt;br&gt;
--
    
- Now, we can perform many of the same transformations using libraries like pandas (python) or tidyverse (R) 
  - The overhead is still challenging to maintain, but modularization, reproducibility, and sensible pipeline design can make it easier
  - Automating these tasks can be easier
  - The tools for automating tasks can be free too

&lt;br&gt;  
--
  
- We can also do much more complex transformations or incorporate more specific architectural details using the flexibility of libraries in python/R that may not be native to SQL
  - Geographies, shape files, distance calculations, etc 
  - Image and file related processing
  - Greater mathematical needs, like ML algorithms, etc 

---

class: middle, center, inverse
# SQLite Meets python and/or R

## Why python and/or R?

## **Connect to a SQLite DB**

## Run SQLite Commands 


---

class: top, left, inverse
# Connect to a SQLite DB

### Python Example

We can easily connect SQLite to python:

```
import pandas as pd
import sqlite3


#set your location, slash direction will change for windows and mac
DB = '/Users/thomas/Documents/GitHub/02-intro_SQL/SQL/FarmersMarket.db' 

#establish your connection
conn = sqlite3.connect(DB, isolation_level=None,
                       detect_types=sqlite3.PARSE_COLNAMES)
```

---

class: top, left, inverse
# Connect to a SQLite DB

### Python Example

Using pandas, we can use this connection to run queries:

```
#run your query, use "\" to allow line breaks
db_df = pd.read_sql_query("SELECT p.*,pc.product_category_name \
                          FROM product p \
                          JOIN product_category pc \
                             ON p.product_category_id = pc.product_category_id"
                          ,conn)
```

--

`db_df` is now available as a dataframe (a specialized type of array in python) for any pandas command

Like extracting a query result to csv:

```
#save
db_df.to_csv('database-py.csv', index=False)
```

---

class: top, left, inverse
# Connect to a SQLite DB

### R Example

We can do the same in R:

```
library(DBI)
library(RSQLite)

#set your location, slash direction will change for windows and mac
DB = '/Users/thomas/Documents/GitHub/02-intro_SQL/SQL/FarmersMarket.db'

#establish your connection
conn &lt;- dbConnect(SQLite(), DB) 
```

---

class: top, left, inverse
# Connect to a SQLite DB

### R Example

And similarly produce a csv, or any other R command we might want to perform on the dataframe:

```
#run your query
db_df &lt;- dbGetQuery(conn, "SELECT p.*,pc.product_category_name
                          FROM product p
                          JOIN product_category pc
                            ON p.product_category_id = pc.product_category_id")

#save
write.csv(db_df, file = "database-R.csv")
```

---

class: top, left, inverse
# Connect to a SQLite DB

(live coding in R)

---

class: middle, center, inverse
**Questions about SQLite connections?**

---

class: middle, center, inverse
# SQLite Meets python and/or R

## Why python and/or R?

## Connect to a SQLite DB

## **Run SQLite Commands**

---

class: left, top, inverse
# Run SQLite in python and/or R

The previous section showed how to connect a SQLite database to python and/or R and interact with tables

**...but you can also run SQLite queries on python and/or R dataframe objects!**

&lt;br&gt;
--

- Both languages offer good support 
    - Often there is corresponding python and/or R syntax to achieve similar results
        - As always with data, there is no prescribed way of doing something!
        
---
class: left, top, inverse
# Run SQLite in python and/or R

### Python Example



```python
import pandas as pd
import pandasql as sql #this allows us to run SQLite queries!

p = "https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv"
penguins = pd.read_csv(p) #create a dataframe
yrly_penguins = sql.sqldf('''SELECT DISTINCT year, COUNT(*) AS count, 
                          SUM(COUNT(*)) OVER (ORDER BY year) AS running_total
                          FROM penguins
                          GROUP BY year''') #run a SQLite query with sqldf()
```

.pull-left.w15[
| year | count | running_total |
|------|-------|---------------|
| 2007 | 110   | 110           |
| 2008 | 114   | 224           |
| 2009 | 120   | 344           |
]

---
class: left, top, inverse
# Run SQLite in python and/or R

### R Example


```r
library(palmerpenguins) #load some data
library(sqldf) #this allows us to run SQLite queries!

penguins &lt;- penguins #create a dataframe
avg_mass &lt;- sqldf('SELECT DISTINCT species, sex, AVG(body_mass_g) as avg_mass
      FROM penguins
      WHERE sex &lt;&gt; "NA"
      GROUP BY species, sex') #run a SQLite query with sqldf()
```

.pull-left.w15[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; species &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; sex &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; avg_mass &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Adelie &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; female &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3368.836 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Adelie &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; male &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4043.493 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Chinstrap &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; female &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3527.206 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Chinstrap &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; male &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3938.971 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Gentoo &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; female &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4679.741 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Gentoo &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; male &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5484.836 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


]

---

class: top, left, inverse
# Run SQLite in python and/or R

(live coding in python)

---

class: middle, center, inverse
**Questions about Running SQLite commands?**

---

class: middle, center, inverse
# SQL and Machine Learning:

## What is Machine Learning?

## SQL's Role

## SQLite Meets python and/or R

## **Connecting the Dots**

---

class: middle, center, inverse
# Connecting the Dots

## End-to-End Machine Learning

## Data Extraction

## Data Manipulation

## Simple Decision Tree (Classification)

## Model Monitoring

---

class: middle, center, inverse
# Connecting the Dots

## **End-to-End Machine Learning**

## Data Extraction

## Data Manipulation

## Simple Decision Tree (Classification)

## Model Monitoring

---

class: top, left, inverse
# Connecting the Dots

### End-to-End Machine Learning

The purpose of this section is to introduce you to an ML example, starting in SQL and finishing with a live model we can test.

--

We'll use the [Palmer Penguins Dataset](https://github.com/allisonhorst/palmerpenguins), developed by Allison Horst. The full history of this dataset and its purpose are available here: [link](https://journal.r-project.org/articles/RJ-2022-020/)

--

The objective will be to predict which species of Palmer Penguin a penguin is, based on bill length, bill depth, flipper length, body mass, sex, and island (i.e. where the penguin was when the data was collected).

--

There are three species: Chinstrap, Gentoo, and Adelie.

.center[ 
&lt;img src="imgs/penguins.png"
     height="200px";&gt;
     
&lt;sup&gt;&lt;sup&gt; Artwork by @allison_horst &lt;/sup&gt;&lt;/sup&gt;
]


---

class: top, left, inverse
# Connecting the Dots

### End-to-End Machine Learning

- Using SQL, load our dataset (penguins) as a csv
- Update the column types so that `bill_length_mm`, `bell_depth_mm`, `flipper_length_mm` are `NUMERIC`, `rowid`, `body_mass_g`, `year` are `INTEGER`, and `species`, `island`, `sex` are `TEXT`
- Connect to our SQLite DB in either python or R (we'll use R in our example)
  - Build a query to extract the data from SQL
- Build a Decision Tree classification model to predict `species`
  - Use tidymodels in R
  - Use SKLearn in python
- Test the model's performance
  - Confusion matrix
- Provide our model new data
  - In our example, we'll create a live API connection in R to our model and provide it with JSON formatted data exported from SQLite 
      - This will be our "monitoring" concept

---

class: middle, center, inverse
# Connecting the Dots

## End-to-End Machine Learning

## **Data Extraction**

## Data Manipulation

## Simple Decision Tree (Classification)

## Model Monitoring

---


class: top, left, inverse
# Connecting the Dots

### Data Extraction

- Build your connection to SQLite
  - python:
  
  ```
  import sqlite3
  #set your location, slash direction will change for windows and mac
  DB = '/Users/thomas/Documents/GitHub/02-intro_SQL/SQL/FarmersMarket.db' 
  #establish your connection
  conn = sqlite3.connect(DB, isolation_level=None,
                         detect_types=sqlite3.PARSE_COLNAMES)
  ```

  - R:
  
  ```
  library(DBI)
  library(RSQLite)
  #set your location, slash direction will change for windows and mac
  DB = '/Users/thomas/Documents/GitHub/02-intro_SQL/SQL/FarmersMarket.db'
  #establish your connection
  conn &lt;- dbConnect(SQLite(), DB)
```

---

class: middle, center, inverse
# Connecting the Dots

## End-to-End Machine Learning

## Data Extraction

## **Data Manipulation**

## Simple Decision Tree (Classification)

## Model Monitoring

---

class: top, left, inverse
# Connecting the Dots

### Data Manipulation

- Write a query:
  - python:
  
  ```
  db_df = pd.read_sql_query("SELECT ... FROM penguins ...",conn)
  ```
  
  - R:
  
  ```
  db_df &lt;- dbGetQuery(conn,"SELECT... FROM penguins ...")
  ```

--

- For our example, only select `species`, `island`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`, `sex`

- And filter out missing values for the variable `sex`
  - `WHERE sex IS NOT NULL`
  - **What other options could we use to replace missing values?**


---

class: middle, center, inverse
# Connecting the Dots

## End-to-End Machine Learning

## Data Extraction

## Data Manipulation

## **Simple Decision Tree (Classification)**

## Model Monitoring

---

class: top, left, inverse
# Connecting the Dots

### Simple Decision Tree (Classification)
- Decision Trees in Machine Learning are like Decision Trees in real life!
- Start with a set of rules to evaluate the results:
  - Does the animal breathe air? 
      - Yes -&gt; Does the animal lay eggs?
          - Yes -&gt; Bird
          - No -&gt; Mammal
      - No -&gt; Fish
- [Metal or Normal?](https://www.youtube.com/watch?v=WPUZ_JaO-go)

&lt;br&gt;
--

- So...we ask the machine to give us a set of rules based on the data
- Decision Trees are useful because they are:
  - Explainable: *humans can understand how the data yielded the result*
  - Flexible: *will ignore irrelevant features, can perform both regression and classification, allows missing data, allows categorical data*
  - Fast: *computationally cheap (usually), especially compared to other ML algorithms*

---

class: top, left, inverse
# Connecting the Dots

### Simple Decision Tree (Classification)

- Decision trees work on **Information Gain**
- Use a top-down, recursive approach to determine the most sensible *"root node"* (the top of the tree, from which we will split)
    - Mathematically, this is based on the ID3, "Iterative Dichotomiser 3" method

&lt;br&gt;
--

  - After determining an appropriate root node (whichever has provided the highest information gain), recursively iterate on the remaining attributes to find the next highest information gain and split again
      - Decision Trees are greedy: this means that there is a good chance that better combinations of rules *can* exist, but ID3 did not test for them

&lt;br&gt;
--

  - Split the newly created nodes again (and again...and again...) until:
      - All values for a given node belong to the same class
      - No values exist without a classification (for all nodes)
      - There are no remaining attributes to partition on
      - The algorithm has been told to stop (e.g. with a parameter)

---

class: top, left, inverse
# Connecting the Dots

### Simple Decision Tree (Classification)

Let's consider an example:

.pull-left.w40[
Some rows in penguins having missing sex:


.center[ 
&lt;img src="imgs/dt_julia_silge.png"
     height="280px";&gt;
     
&lt;sup&gt;&lt;sup&gt; Julia Silge @ Posit &lt;/sup&gt;&lt;/sup&gt;
]

]

--

.pull-right.w40[

We can estimate sex based on the following ruleset:

- body_mass_g &lt; 3717
  -  Yes -&gt; bill_length_mm &lt; 48
      - Yes -&gt; female (88% meeting this criteria were female)
      - No -&gt; male (62% male)
  - No -&gt; bill_length_mm &lt; 49
      - Yes -&gt; species either Chinstrap or Gentoo
          - Yes -&gt; body_mass_g &lt; 4988
              - Yes -&gt; female (96% female)
              - No -&gt; male (79% male)
          - No -&gt; male (88% male)
      - No -&gt; male (97% male)
]

---

class: top, left, inverse
# Connecting the Dots

(live end-to-end machine learning in R, part 1)

---

class: top, left, inverse
# Connecting the Dots

### Simple Decision Tree (Classification)

**How did our model do?**

--

Overall accuracy was 93%!
--

.pull-left.w40[
84 penguins were divided as follows:

.center[
&lt;img src="imgs/confmat_penguins.png"
     height="170px";&gt;
 
&lt;sup&gt;&lt;sup&gt; (we call this a "confusion matrix") &lt;/sup&gt;&lt;/sup&gt;
]
]
--

.pull-left.w20[

```
```

]

.pull-left.w30[

And our estimated ruleset was:

.center[ 
&lt;img src="imgs/dt_penguins.png"
     height="250px";&gt;
     
]
]

---

class: middle, center, inverse
# Connecting the Dots

## End-to-End Machine Learning

## Data Extraction

## Data Manipulation

## Simple Decision Tree (Classification)

## **Model Monitoring**

---

class: top, left, inverse
# Connecting the Dots

### Model Monitoring

- Model predictions are based on **historical data**
  - There is no certainty that future data will appear the same as historical data
  - Models won't adjust to changes in the data

&lt;br&gt;
--

- Most algorithms are poor at explaining how they arrived at a prediction
  - Some models are "white-box" by nature (like decision trees), while others are "black-box" (like deep-learning neural nets)
  - The inherent explainability of white-box models makes them easier to evaluate over time

&lt;br&gt;
--

- When models change over time, we call this *model drift*
- There are two types of drift:
  - **Concept Drift**: the way the model 'understood' things to relate to each other has changed, in a way the model doesn't know, so the model is no longer helpful 
  - **Data Drift**: information that the model has to deal with now is too different from the information the model has previously seen, so the model is no longer helpful

---

class: top, left, inverse
# Connecting the Dots

### Model Monitoring

- To observe model drift, we need to gather new data and compare how the model is performing
- If metrics (like accuracy, but there are others too) are changing over time, we are observing drift

&lt;br&gt;
--

- One type of new data we can collect is called "ground truth"
- Suppose we have an algorithm predicting whether or not a credit card transaction is fraudulent:
  - Each time the algorithm flags a transaction as fraudulent, a customer service representative calls the credit card holder
  - For instances where fraud occurred, the representative can mark the algorithm as correct
  - For instances where the transaction was legitimate, the representative can mark the algorithm as incorrect
  - **What's missing from this?**
  
&lt;br&gt;
--

- Ground truth, even in cases where it may be difficult to obtain, can greatly improve a model's usefulness and its longevity 
- SQL is a perfect place to store ground truth data!
  - e.g. the customer service representative might have access to a frontend UI that allows them to mark fraudulent/non-fraudulent transactions, which are then stored in a SQL backend
  
---

class: top, left, inverse
# Connecting the Dots

### Model Monitoring

- We claimed our penguin model performed with 93% accuracy
- Because this class can't go and collect more penguin data, it'd be challenging to evaluate the model's accuracy over time 🥶
- However, we can emulate this process, and make the most of our SQL skills!

&lt;br&gt;
--

- Using the `vetiver` and `plumber` packages, and following fantastic instructions from [Julia Silge's  blog](https://juliasilge.com/blog/), we'll create a live connection to our model
- We'll then export a subset of penguins data (or make up our own? sure!) as JSON and watch our model predict!

---

class: top, left, inverse
# Connecting the Dots

### Model Monitoring

The API produces predictions as JSON, so we can input these predicted values and compare to actual values
  
```
DROP TABLE IF EXISTS temp.[predicted_penguins];

CREATE TABLE IF NOT EXISTS temp.[predicted_penguins]
(predictions BLOB);


INSERT INTO temp.[predicted_penguins](predictions)
VALUES('[{".pred_class": "Adelie"}]');
	

SELECT pen.*, JSON_EXTRACT(value, '$.pred_class') AS prediction

FROM (SELECT *
	    FROM predicted_penguins,JSON_EACH(predicted_penguins.predictions,'$')) x

-- we need to generate a row number to keep track of which prediction was which
JOIN penguins_json_export pen ON key = row_number  

```

--

We've now gone end-to-end: from SQL to ML and back to SQL again!

---

class: top, left, inverse
# Connecting the Dots

(live end-to-end machine learning in R, part 2)

---

class: middle, center, inverse
**Lingering Questions?**


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "style_solarized_light",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%",
"ratio": "16:9",
"seal": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
